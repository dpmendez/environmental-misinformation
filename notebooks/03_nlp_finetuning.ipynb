{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))  # adjust '..' as needed\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate # Hugging Face splits metrics into a separate package\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import plotly.express as px\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Load CSVs\n",
    "# -------------------------------\n",
    "train_df = pd.read_csv(\"../data/train_data.csv\").dropna()\n",
    "val_df   = pd.read_csv(\"../data/val_data.csv\").dropna()\n",
    "test_df  = pd.read_csv(\"../data/test_data.csv\").dropna()\n",
    "\n",
    "# Get all unique labels from training set\n",
    "label_names = sorted(train_df['label'].unique())  # e.g. ['FAKE','REAL','NEUTRAL','OTHER']\n",
    "label2id = {name: i for i, name in enumerate(label_names)}\n",
    "\n",
    "# Apply mapping to all splits\n",
    "for df in [train_df, val_df, test_df]:\n",
    "    df['label'] = df['label'].map(label2id)\n",
    "\n",
    "X_train = train_df['text']\n",
    "y_train = train_df['label'].values\n",
    "\n",
    "X_val = val_df['text']\n",
    "y_val = val_df['label'].values\n",
    "\n",
    "X_test = test_df['text']\n",
    "y_test = test_df['label'].values\n",
    "\n",
    "\n",
    "# For Hugging Face dataset format\n",
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_dict({'text': X_train, 'label': y_train})\n",
    "val_dataset   = Dataset.from_dict({'text': X_val, 'label': y_val})\n",
    "test_dataset  = Dataset.from_dict({'text': X_test, 'label': y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Quick test: Pretrained pipeline (no training)\n",
    "# -------------------------------\n",
    "from transformers import pipeline\n",
    "classifier = pipeline(\"text-classification\", model=\"distilbert-base-uncased\", device=0)  # device=0 for GPU\n",
    "\n",
    "sample_texts = [\"The climate is changing faster than expected.\", \"Some claims about climate are fake.\"]\n",
    "print(\"Quick predictions:\", classifier(sample_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Fine-tuning a pretrained model\n",
    "# -------------------------------\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model     = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(np.unique(y_train)))\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import Trainer\n",
    "\n",
    "# Compute class weights from training data\n",
    "class_counts = np.bincount(y_train)  # count per numeric label\n",
    "total = sum(class_counts)\n",
    "weights = [total/c for c in class_counts]  # inverse frequency weighting\n",
    "weights = torch.tensor(weights, dtype=torch.float)\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Map back to label names\n",
    "id2label = {i: name for name, i in label2id.items()}\n",
    "\n",
    "freqs = class_counts.tolist()\n",
    "wts   = weights.tolist()\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "    go.Bar(name=\"Class Frequency\", x=[id2label[i] for i in range(len(freqs))], y=freqs),\n",
    "    go.Bar(name=\"Class Weight\",    x=[id2label[i] for i in range(len(wts))], y=wts)\n",
    "])\n",
    "\n",
    "fig.update_layout(\n",
    "    barmode='group',\n",
    "    title=\"Class Frequencies vs. Weights\",\n",
    "    xaxis_title=\"Label\",\n",
    "    yaxis_title=\"Value\",\n",
    "    template=\"plotly_white\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenization\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True, max_length=128)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "val_dataset   = val_dataset.map(tokenize, batched=True)\n",
    "test_dataset  = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "val_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "# -------------------------------\n",
    "# Train / fine-tune\n",
    "# -------------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=1e-5,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    report_to=\"none\"   # <-- disables wandb, tensorboard, etc.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import compute_metrics\n",
    "from src.models import WeightedTrainer\n",
    "import torch.nn as nn\n",
    "\n",
    "#trainer = Trainer(\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute optimal threshold\n",
    "\n",
    "from src.threshold import find_optimal_threshold, classify_with_threshold\n",
    "\n",
    "false_label_id = label2id[\"LIKELY_FALSE\"]\n",
    "\n",
    "best_threshold, fig = find_optimal_threshold(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    val_dataset=val_dataset,\n",
    "    false_label_id=false_label_id,\n",
    "    harmful_cost=1.0,\n",
    "    benign_cost=0.3\n",
    ")\n",
    "\n",
    "print(\"Optimal Threshold:\", best_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Evaluation on test set\n",
    "# -------------------------------\n",
    "preds_output = trainer.predict(test_dataset)\n",
    "y_pred = np.argmax(preds_output.predictions, axis=1)\n",
    "\n",
    "# Bring back original labels\n",
    "id2label = {v:k for k,v in label2id.items()}\n",
    "y_test_labels = [id2label[i] for i in y_test]\n",
    "y_pred_labels = [id2label[i] for i in y_pred]\n",
    "\n",
    "print(classification_report(y_test_labels, y_pred_labels, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Override decision rule using optimal threshold\n",
    "# -----------------------------------------------\n",
    "from scipy.special import softmax\n",
    "\n",
    "# Convert logits to probabilities\n",
    "logits = preds_output.predictions  \n",
    "probs = softmax(logits, axis=1)\n",
    "p_positive = probs[:, 1]  # probability of \"fake\" or class 1\n",
    "\n",
    "# Apply optimal threshold\n",
    "threshold = best_threshold\n",
    "# y_pred = (p_positive >= threshold).astype(int)\n",
    "y_pred = (p_positive >= 0.35).astype(int)\n",
    "\n",
    "# Convert numeric IDs to label names\n",
    "y_pred_labels = [id2label[i] for i in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Confusion Matrix\n",
    "# -------------------------------\n",
    "from src.viz import *\n",
    "cm = confusion_matrix(y_test_labels, y_pred_labels)\n",
    "cm_df = pd.DataFrame(cm, index=np.unique(y_test_labels), columns=np.unique(y_test_labels))\n",
    "\n",
    "fig = px.imshow(cm_df,\n",
    "                text_auto=True,\n",
    "                labels=dict(x=\"Predicted\", y=\"Actual\", color=\"Count\"),\n",
    "                color_continuous_scale=\"Blues\",\n",
    "                title=\"Confusion Matrix\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Word Clouds for each label\n",
    "# -------------------------------\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# First, create a DataFrame with text + predicted label\n",
    "test_texts = test_df[\"text\"].tolist()\n",
    "pred_labels = y_pred_labels\n",
    "\n",
    "df_pred = pd.DataFrame({\n",
    "    \"text\": test_texts,\n",
    "    \"pred\": pred_labels\n",
    "})\n",
    "\n",
    "label_names = np.unique(y_pred_labels)\n",
    "\n",
    "# Generate a word cloud per label\n",
    "for lbl in label_names:\n",
    "    texts = \" \".join(df_pred[df_pred[\"pred\"]==lbl][\"text\"])\n",
    "\n",
    "    # Create the word cloud\n",
    "    wc = WordCloud(width=800, height=400, background_color=\"white\", colormap=\"coolwarm\").generate(texts)\n",
    "\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Word Cloud for label: {lbl}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "df_pred = pd.DataFrame({\n",
    "    \"text\": test_texts,\n",
    "    \"pred\": y_pred_labels\n",
    "})\n",
    "\n",
    "# Colors per label\n",
    "label_colors = {\n",
    "    \"LIKELY_TRUE\": \"blue\",\n",
    "    \"LIKELY_FALSE\": \"red\"\n",
    "    # \"SUPPORTS\": \"seagreen\",\n",
    "    # \"REFUTES\": \"crimson\",\n",
    "    # \"NEUTRAL\": \"gray\",\n",
    "    # \"DISPUTED\": \"orange\"\n",
    "}\n",
    "\n",
    "for lbl in np.unique(y_pred_labels):\n",
    "    texts = \" \".join(df_pred[df_pred[\"pred\"]==lbl][\"text\"])\n",
    "    if not texts.strip():\n",
    "        print(f\"No predictions for label {lbl}, skipping word cloud.\")\n",
    "        continue\n",
    "\n",
    "    word_freq = Counter(texts.split())\n",
    "\n",
    "    wc = WordCloud(\n",
    "        width=800,\n",
    "        height=400,\n",
    "        background_color=\"white\",\n",
    "        max_words=100,\n",
    "        color_func=lambda *args, **kwargs: label_colors[lbl]  # pass label directly here\n",
    "    ).generate_from_frequencies(word_freq)\n",
    "\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Word Cloud for label: {lbl}\", fontsize=16)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environmental-misinformation (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
