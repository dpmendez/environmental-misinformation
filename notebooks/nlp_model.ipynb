{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))  # adjust '..' as needed\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate # Hugging Face splits metrics into a separate package\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import plotly.express as px\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Load CSVs\n",
    "# -------------------------------\n",
    "train_df = pd.read_csv(\"../data/train_data.csv\").dropna()\n",
    "val_df   = pd.read_csv(\"../data/val_data.csv\").dropna()\n",
    "test_df  = pd.read_csv(\"../data/test_data.csv\").dropna()\n",
    "\n",
    "# Get all unique labels from training set\n",
    "label_names = sorted(train_df['label'].unique())  # e.g. ['FAKE','REAL','NEUTRAL','OTHER']\n",
    "label2id = {name: i for i, name in enumerate(label_names)}\n",
    "id2label = {i: name for name, i in label2id.items()}\n",
    "\n",
    "print('label_names: ', label_names)\n",
    "print('label2id: ', label2id)\n",
    "print('id2label: ', id2label)\n",
    "\n",
    "# Apply mapping to all splits\n",
    "for df in [train_df, val_df, test_df]:\n",
    "    df['label'] = df['label'].map(label2id)\n",
    "\n",
    "X_train = train_df['text']\n",
    "y_train = train_df['label'].values\n",
    "\n",
    "X_val = val_df['text']\n",
    "y_val = val_df['label'].values\n",
    "\n",
    "X_test = test_df['text']\n",
    "y_test = test_df['label'].values\n",
    "\n",
    "# For Hugging Face dataset format\n",
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_dict({'text': X_train, 'label': y_train})\n",
    "val_dataset   = Dataset.from_dict({'text': X_val, 'label': y_val})\n",
    "test_dataset  = Dataset.from_dict({'text': X_test, 'label': y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get class weights -- there's an imbalance in the dataset\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "print(\"Final class weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Fine-tuning pretrained model\n",
    "# -------------------------------\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import pipeline, Trainer\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"; model_name_out = \"distilbert-base-uncased\"\n",
    "# model_name = \"bert-base-uncased\"; model_name_out = \"bert-base-uncased\"\n",
    "# model_name = \"google/electra-base-discriminator\"; model_name_out = \"electra-base-discriminator\"\n",
    "# model_name = \"answerdotai/ModernBERT-base\"; model_name_out = \"Modernbert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model     = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(np.unique(y_train)))\n",
    "\n",
    "# Tokenization\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True, max_length=128)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "val_dataset   = val_dataset.map(tokenize, batched=True)\n",
    "test_dataset  = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "val_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=20,\n",
    "    per_device_eval_batch_size=20,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=1e-5,\n",
    "    load_best_model_at_end=True, # keep the best model\n",
    "    metric_for_best_model=\"f1_weighted\",\n",
    "    greater_is_better=True,\n",
    "    report_to=\"none\", # disable wandb, tensorboard, etc.\n",
    "    dataloader_pin_memory=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import compute_metrics\n",
    "from src.models import WeightedTrainer\n",
    "import torch.nn as nn\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "#trainer = Trainer(\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    class_weights=class_weights,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# save best model\n",
    "model_path = \"./results/transformer_model/\" + model_name_out + \"/\"\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "trainer.save_model(model_path)\n",
    "tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute optimal threshold\n",
    "from src.threshold import find_optimal_threshold, plot_threshold_optimization\n",
    "\n",
    "false_label_id = label2id[\"LIKELY_FALSE\"]\n",
    "\n",
    "result = find_optimal_threshold(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    val_dataset=val_dataset,\n",
    "    false_label_id=false_label_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_threshold = result[\"best_threshold\"]\n",
    "plot_threshold_optimization(result, 'cost')\n",
    "\n",
    "print(\"Optimal Threshold:\", best_threshold)\n",
    "\n",
    "# Save threshold and label maps\n",
    "import json\n",
    "with open(os.path.join(model_path, \"threshold.json\"), \"w\") as f:\n",
    "    json.dump({\"best_threshold\": best_threshold}, f)\n",
    "\n",
    "with open(os.path.join(model_path, \"label_map.json\"), \"w\") as f:\n",
    "    json.dump({'label2id':label2id, 'id2label':id2label},f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Evaluation on val set (apply thresholded predictions and compute metrics)\n",
    "# -------------------------------\n",
    "from scipy.special import softmax\n",
    "from src.utils import compute_metrics\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Get raw predictions from the trainer\n",
    "preds_output = trainer.predict(val_dataset)\n",
    "logits = preds_output.predictions  # shape (N, num_labels)\n",
    "probs = softmax(logits, axis=1)\n",
    "\n",
    "# Probability of the 'likely_false' class (assumed to be class 0)\n",
    "p_false = probs[:, 0]\n",
    "\n",
    "# Use the previously computed best threshold\n",
    "threshold = best_threshold\n",
    "y_pred = np.where(p_false >= threshold, 0, 1)\n",
    "\n",
    "# Map numeric preds to label names for downstream reporting\n",
    "id2label_names = {v: k for k, v in label2id.items()}  # numeric to name\n",
    "y_pred_labels = [id2label_names[int(i)] for i in y_pred]\n",
    "y_val_labels = [id2label_names[int(i)] for i in y_val]\n",
    "\n",
    "# Compute metrics using the updated compute_metrics (accepts 1D preds)\n",
    "metrics_thresholded = compute_metrics((y_pred, np.array(y_val)))\n",
    "print('Metrics (compute_metrics) for thresholded predictions:')\n",
    "print(json.dumps(metrics_thresholded, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Confusion Matrix\n",
    "# -------------------------------\n",
    "from src.viz import * \n",
    "\n",
    "label_order = [\"LIKELY_FALSE\", \"LIKELY_TRUE\"]\n",
    "\n",
    "# Overall confusion matrix\n",
    "plotly_confusion_matrix(y_val_labels, y_pred_labels, label_order, title=\"Overall Confusion Matrix\")\n",
    "\n",
    "cm = confusion_matrix(y_val_labels, y_pred_labels)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print('tn, fp, fn, tp:', tn, fp, fn, tp)\n",
    "\n",
    "# cm_df = pd.DataFrame(cm, index=np.unique(y_val_labels), columns=np.unique(y_val_labels))\n",
    "# fig = px.imshow(cm_df,\n",
    "#                 text_auto=True,\n",
    "#                 labels=dict(x=\"Predicted\", y=\"Actual\", color=\"Count\"),\n",
    "#                 color_continuous_scale=\"Blues\",\n",
    "#                 title=\"Confusion Matrix\")\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Word Clouds for each label\n",
    "# -------------------------------\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# First, create a DataFrame with text + predicted label\n",
    "val_texts = val_df[\"text\"].tolist()\n",
    "pred_labels = y_pred_labels\n",
    "\n",
    "df_pred = pd.DataFrame({\n",
    "    \"text\": val_texts,\n",
    "    \"pred\": pred_labels\n",
    "})\n",
    "\n",
    "label_names = np.unique(y_pred_labels)\n",
    "\n",
    "# Generate a word cloud per label\n",
    "for lbl in label_names:\n",
    "    texts = \" \".join(df_pred[df_pred[\"pred\"]==lbl][\"text\"])\n",
    "\n",
    "    # Create the word cloud\n",
    "    wc = WordCloud(width=800, height=400, background_color=\"white\", colormap=\"coolwarm\").generate(texts)\n",
    "\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Word Cloud for label: {lbl}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "df_pred = pd.DataFrame({\n",
    "    \"text\": val_texts,\n",
    "    \"pred\": y_pred_labels\n",
    "})\n",
    "\n",
    "# Colors per label\n",
    "label_colors = {\n",
    "    \"LIKELY_TRUE\": \"blue\",\n",
    "    \"LIKELY_FALSE\": \"red\"\n",
    "    # \"SUPPORTS\": \"seagreen\",\n",
    "    # \"REFUTES\": \"crimson\",\n",
    "    # \"NEUTRAL\": \"gray\",\n",
    "    # \"DISPUTED\": \"orange\"\n",
    "}\n",
    "\n",
    "for lbl in np.unique(y_pred_labels):\n",
    "    texts = \" \".join(df_pred[df_pred[\"pred\"]==lbl][\"text\"])\n",
    "    if not texts.strip():\n",
    "        print(f\"No predictions for label {lbl}, skipping word cloud.\")\n",
    "        continue\n",
    "\n",
    "    word_freq = Counter(texts.split())\n",
    "\n",
    "    wc = WordCloud(\n",
    "        width=800,\n",
    "        height=400,\n",
    "        background_color=\"white\",\n",
    "        max_words=100,\n",
    "        color_func=lambda *args, **kwargs: label_colors[lbl]  # pass label directly here\n",
    "    ).generate_from_frequencies(word_freq)\n",
    "\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Word Cloud for label: {lbl}\", fontsize=16)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environmental-misinformation (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
