{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Baseline model for claim classification\n",
    "\n",
    "In this notebook, we'll explore basic ML models to train and make inferences about the veracity of environment related claims. These models, such as _linear regression_ and _naive bayes_, are simplier, faster and more easily interpretable than more advanced deep learning approaches. \n",
    "\n",
    "The results from this exercise will be used as reference point for the claim classification project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import required libraries\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "Import data from previously produced csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../data/train_data.csv\")\n",
    "test_df = pd.read_csv(\"../data/test_data.csv\")\n",
    "val_df = pd.read_csv(\"../data/val_data.csv\")\n",
    "\n",
    "# remove nans. they cant be vectorized\n",
    "train_df = train_df.dropna()\n",
    "test_df = test_df.dropna()\n",
    "val_df = val_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Separate features and labels\n",
    "\n",
    "Weâ€™ll train on _clean_text_ and predict _label_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_list = ['text']\n",
    "y_list = ['label']\n",
    "\n",
    "x_train, y_train = train_df[x_list], train_df[y_list]\n",
    "x_test, y_test   = test_df[x_list], test_df[y_list]\n",
    "x_val, y_val     = val_df[x_list], val_df[y_list]\n",
    "\n",
    "y_train = y_train.values.ravel() # flaten so it's labels are 1D\n",
    "y_test  = y_test.values.ravel() # flaten so it's labels are 1D\n",
    "y_val   = y_val.values.ravel() # flaten so it's labels are 1D\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Baseline pipeline\n",
    "\n",
    "We'll use **TF-IDF** (Term Frequency - Inverse Document Frequency) with **Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "#### Model parameter optimization\n",
    "Through grid scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, precision_recall_fscore_support\n",
    "from sklearn.svm import LinearSVC\n",
    "import pandas as pd\n",
    "\n",
    "param_grid = [\n",
    "    {\"ngram_range\": (1,1), \"max_features\": 5000, \"class_weight\": None},\n",
    "    {\"ngram_range\": (1,2), \"max_features\": 5000, \"class_weight\": None},\n",
    "    {\"ngram_range\": (1,2), \"max_features\": 10000, \"class_weight\": None},\n",
    "    {\"ngram_range\": (1,2), \"max_features\": 20000, \"class_weight\": None},\n",
    "    {\"ngram_range\": (1,2), \"max_features\": 5000,  \"class_weight\": \"balanced\"},\n",
    "    {\"ngram_range\": (1,2), \"max_features\": 10000, \"class_weight\": \"balanced\"},\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for params in param_grid:\n",
    "    clf_temp = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(\n",
    "            ngram_range=params[\"ngram_range\"],\n",
    "            max_features=params[\"max_features\"],\n",
    "            stop_words=\"english\"\n",
    "        )),\n",
    "        (\"svc\", LinearSVC(\n",
    "            max_iter=2000,\n",
    "            class_weight=params[\"class_weight\"]\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    clf_temp.fit(x_train[\"text\"], y_train)\n",
    "    y_pred_temp = clf_temp.predict(x_val[\"text\"])\n",
    "\n",
    "    acc = accuracy_score(y_val, y_pred_temp)\n",
    "    acc_balanced = balanced_accuracy_score(y_val, y_pred_temp)\n",
    "    f1_macro = f1_score(y_val, y_pred_temp, average=\"macro\")\n",
    "    f1_weighted = f1_score(y_val, y_pred_temp, average=\"weighted\")\n",
    "    results.append({\n",
    "        \"ngram_range\": params[\"ngram_range\"],\n",
    "        \"max_features\": params[\"max_features\"],\n",
    "        \"class_weight\": params[\"class_weight\"],\n",
    "        \"accuracy\": acc,\n",
    "        \"f1 macro\": f1_macro,\n",
    "        \"accuracy balanced\": acc_balanced,\n",
    "        \"f1 weighted\": f1_weighted\n",
    "    })\n",
    "\n",
    "# Put in table\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "Chose parameters based on tradeoff between accuracy and f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "ngram_range=(1,2)\n",
    "class_weight=\"balanced\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Baseline performance\n",
    "\n",
    "Chose a model between logistic regression, random forest, and xgboost based on overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models_xgb import *\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# XGBooost needs encoded labels\n",
    "le = LabelEncoder()\n",
    "y_train_enc = le.fit_transform(y_train)\n",
    "y_val_enc  = le.transform(y_val)\n",
    "\n",
    "print(\"Classes mapping:\", dict(zip(le.classes_, le.transform(le.classes_))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import compute_metrics\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "log_model = train_classic_model(x_train, y_train_enc, \"logreg\", ngram_range=ngram_range, max_features=max_features, class_weight=class_weight)\n",
    "rf_model  = train_classic_model(x_train, y_train_enc, \"rf\", ngram_range=ngram_range, max_features=max_features, class_weight=class_weight)\n",
    "svc_model = train_classic_model(x_train, y_train_enc, \"svc\", ngram_range=ngram_range, max_features=max_features, class_weight=class_weight)\n",
    "xgb_model = train_classic_model(x_train, y_train_enc, \"xgb\", ngram_range=ngram_range, max_features=max_features, class_weight=class_weight)\n",
    "\n",
    "results = []\n",
    "\n",
    "models = {\n",
    "        \"log\": log_model, # Logistic Regression\n",
    "        \"rf\": rf_model, # Random Forest\n",
    "        \"svc\": svc_model, # Linear SVC\n",
    "        \"xgb\": xgb_model # XGBoost\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    \n",
    "    # save model\n",
    "    model_path = \"./results/baseline_model/\"+name+\"/\"\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    joblib.dump(model, os.path.join(model_path,name+\".joblib\"))\n",
    "\n",
    "    preds = model.predict(x_val)\n",
    "\n",
    "    acc = accuracy_score(y_val_enc, preds)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_val_enc, preds, average=\"weighted\", zero_division=0)\n",
    "    # Compute metrics using the updated compute_metrics (accepts 1D preds)\n",
    "    metrics_weighted = compute_metrics((preds, np.array(y_val_enc)))\n",
    "    print(f\"Metrics for {name} (weighted):\")\n",
    "    print(json.dumps(metrics_weighted, indent=2))\n",
    "\n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": round(acc, 3),\n",
    "        \"Precision\": round(prec, 3),\n",
    "        \"Recall\": round(rec, 3),\n",
    "        \"F1\": round(f1, 3)\n",
    "    })\n",
    "\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "Picking model to set baseline. Might re-optimize parameters later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.viz import * \n",
    "from src.scoring import get_false_class_scores\n",
    "from src.threshold_xgb import find_optimal_threshold_from_scores\n",
    "\n",
    "for name, model in models.items():\n",
    "\n",
    "    # avoid hard encoded 0.5 threshold from sklearnt\n",
    "    scores_val = get_false_class_scores(model, x_val, name) # get scores for possibly_false\n",
    "    all_probs = scores_val \n",
    "\n",
    "    result = find_optimal_threshold_from_scores(\n",
    "        y_true=y_val_enc,\n",
    "        scores=scores_val,\n",
    "        false_label_id=0,\n",
    "        return_curve=True\n",
    "    )\n",
    "    best_threshold = result[\"best_threshold\"]\n",
    "    print(\"best threshold \", name, \" :\", best_threshold)\n",
    "\n",
    "    # y_pred = np.where(scores_val >= best_threshold, 0, 1)\n",
    "    y_pred = np.where(scores_val >= 0.5, 0, 1)\n",
    "\n",
    "    model_path = \"./results/baseline_model/\"+this_name+\"/\"\n",
    "\n",
    "    # Save threshold and label maps\n",
    "    import json\n",
    "    with open(os.path.join(model_path, \"threshold.json\"), \"w\") as f:\n",
    "        json.dump({\"best_threshold\": best_threshold}, f)\n",
    "\n",
    "    # Decode back into strings\n",
    "    y_pred_str = le.inverse_transform(y_pred)\n",
    "\n",
    "    # Same for y_val if we want consistency\n",
    "    y_val_str = le.inverse_transform(y_val_enc)\n",
    "\n",
    "    acc = accuracy_score(y_val_str, y_pred_str)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_val_str, y_pred_str, average=\"weighted\", zero_division=0)\n",
    "            \n",
    "    # Classification Report\n",
    "    print(classification_report(y_val_str, y_pred_str, zero_division=0))\n",
    "\n",
    "    labels_in_val = list(np.unique(y_val_str))\n",
    "\n",
    "    # Overall confusion matrix\n",
    "    plotly_confusion_matrix(y_val_str, y_pred_str, labels=labels_in_val, title=\"Overall Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "#### Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "#### Feature importance\n",
    "\n",
    "Coefficients tell us what words are most indicative of _FAKE_ vs _REAL_ claims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import * \n",
    "\n",
    "fw = get_feature_importance(rf_model, top_n=20)   # works with RF, XGB or LogReg\n",
    "plot_feature_importance(fw, \"Random Forest Feature Importance\")\n",
    "plot_wordcloud(fw, model_type=\"rf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-xgb (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
